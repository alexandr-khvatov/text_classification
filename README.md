# text classification project   
| Model\\Metrics %          | **Acc.** | **R** | **P** | **F1** | **R<sub>(macro)</sub>** | **P<sub>(macro)</sub>** | **F1<sub>(macro)</sub>** | Size MB |
| -------------------------- | ------------ | ----- | ----- | ------ | ---------------- | ---------------- | ----------------- | --------- |
| Log. Reg.<sub> (baseline)</sub>  | 87.37        | 70.03 | 90.75 | 79.06  | 83.18            | 88.46            | 85.01             | \-        |
| Glove - BiLSTM             | 90.39        | 88.39 | 83.17 | 85.70  | 89.87            | 88.69            | 89.23             | 65.56     |
| FastText \- BiLSTM         | 88.24        | 86.47 | 79.30 | 82.73  | 87.78            | 86.23            | 86.91             | 70.29     |
| Glove – CNN\-GRU           | 90.01        | 89.67 | 81.51 | 85.40  | 89.92            | 88.13            | 88.90             | 55.70     |
| FastText – CNN\-GRU        | 88.00        | 81.47 | 81.64 | 81.56  | 86.31            | 86.35            | 86.33             | 60.44     |
| rubert-tiny<sub> (fine-tuning)</sub>  | 89.84        | 86.37 | 83.09 | 84.70  | 88.94            | 88.19            | 88.55             | 45.20     |
| rubert-tiny2<sub> (fine-tuning)</sub> | 92.47        | 91.05 | 86.54 | 88.74  | 92.11            | 91.05            | 91.54             | 112.00    |
